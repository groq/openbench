---
title: Benchmarks Catalog
description: Complete catalog of all available benchmarks with evaluation commands
---

## Featured

<CardGroup cols={3}>
  <Card title="MMLU" icon="graduation-cap">
    **57 subjects** · Multiple choice · Knowledge & reasoning

    Massive Multitask Language Understanding - 57 academic subjects from the cais/mmlu dataset
    
    ```bash
    bench eval mmlu
    ```
  </Card>
  
  <Card title="GPQA Diamond" icon="diamond">
    **Graduate-level** · Science · Multiple choice

    Graduate-level Google-Proof Q&A in biology, chemistry, and physics
    
    ```bash
    bench eval gpqa_diamond
    ```
  </Card>
  
  <Card title="HumanEval" icon="code">
    **Code generation** · Execution

    Code generation benchmark with 164 programming problems
    
    ```bash
    bench eval humaneval
    ```
  </Card>
  
  <Card title="SimpleQA" icon="message-question">
    **Factuality** · Model-Graded

    Measuring short-form factuality in large language models with simple Q&A pairs
    
    ```bash
    bench eval simpleqa
    ```
  </Card>
  
  <Card title="SciCode" icon="flask">
    **Science** · Code generation

    Scientific computing and programming challenges
    
    ```bash
    bench eval scicode
    ```
  </Card>
  
  <Card title="GraphWalks" icon="hexagon-nodes">
    **Graphs** · Long-context · Reasoning

    Multi-hop reasoning on graphs - both BFS and parent finding tasks

    ```bash
    bench eval graphwalks
    ```
  </Card>
</CardGroup>

## Complete Benchmark Catalog

| Benchmark | Description | Command |
|-----------|-------------|---------|
| AIME 2023 I | American Invitational Mathematics Examination 2023 (First) | `bench eval aime_2023_I` |
| AIME 2023 II | American Invitational Mathematics Examination 2023 (Second) | `bench eval aime_2023_II` |
| AIME 2024 | American Invitational Mathematics Examination 2024 (Combined I & II) | `bench eval aime_2024` |
| AIME 2024 I | American Invitational Mathematics Examination 2024 (First) | `bench eval aime_2024_I` |
| AIME 2024 II | American Invitational Mathematics Examination 2024 (Second) | `bench eval aime_2024_II` |
| AIME 2025 | American Invitational Mathematics Examination 2025 | `bench eval aime_2025` |
| AIME 2025 II | American Invitational Mathematics Examination 2025 (Second) | `bench eval aime_2025_II` |
| BoolQ | BoolQ: A Question Answering Dataset for Boolean Reasoning | `bench eval boolq` |
| BrowseComp | A Simple Yet Challenging Benchmark for Browsing Agents - evaluates model performance on browsing-related tasks | `bench eval browsecomp` |
| BRUMO 2025 | Bruno Mathematical Olympiad 2025 | `bench eval brumo_2025` |
| ClockBench | Clock benchmark - time-based reasoning tasks | `bench eval clockbench` |
| CTI-Bench | Comprehensive evaluation framework for cyber threat intelligence understanding with 4 tasks: knowledge questions, vulnerability classification, CVSS scoring, and technique extraction | `bench eval cti_bench` |
| CTI-Bench ATE | Extracting MITRE ATT&CK techniques from malware and threat descriptions | `bench eval cti_bench_ate` |
| CTI-Bench MCQ | Multiple-choice questions evaluating understanding of CTI standards, threats, detection strategies, and best practices using authoritative sources like NIST and MITRE | `bench eval cti_bench_mcq` |
| CTI-Bench RCM | Mapping CVE descriptions to CWE categories to evaluate vulnerability classification ability | `bench eval cti_bench_rcm` |
| CTI-Bench VSP | Calculating CVSS scores from vulnerability descriptions to assess severity evaluation skills | `bench eval cti_bench_vsp` |
| DetailBench | Tests whether LLMs notify users about wrong facts in a text while they are tasked to translate said text | `bench eval detailbench` |
| DROP | Reading comprehension benchmark requiring discrete reasoning over paragraphs (arithmetic, counting, sorting) | `bench eval drop` |
| GMCQ | GitHub Multiple Choice Questions | `bench eval rootly_gmcq` |
| GPQA Diamond | Graduate-level Google-Proof Q&A in biology, chemistry, and physics | `bench eval gpqa_diamond` |
| GraphWalks | Multi-hop reasoning on graphs - both BFS and parent finding tasks | `bench eval graphwalks` |
| GraphWalks BFS | Multi-hop reasoning on graphs - BFS traversal tasks only | `bench eval graphwalks_bfs` |
| GraphWalks Parents | Multi-hop reasoning on graphs - parent finding tasks only | `bench eval graphwalks_parents` |
| HealthBench | Medical dialogue evaluation using physician-created rubrics for assessing healthcare conversations | `bench eval healthbench` |
| HealthBench Consensus | Medical dialogue cases with strong physician consensus on appropriate responses | `bench eval healthbench_consensus` |
| HealthBench Hard | Most challenging medical dialogue cases from HealthBench requiring nuanced medical knowledge | `bench eval healthbench_hard` |
| HMMT Feb 2023 | Harvard-MIT Mathematics Tournament February 2023 | `bench eval hmmt_feb_2023` |
| HMMT Feb 2024 | Harvard-MIT Mathematics Tournament February 2024 | `bench eval hmmt_feb_2024` |
| HMMT Feb 2025 | Harvard-MIT Mathematics Tournament February 2025 | `bench eval hmmt_feb_2025` |
| HumanEval | Code generation benchmark with 164 programming problems | `bench eval humaneval` |
| Humanity's Last Exam | Multi-modal benchmark at the frontier of human knowledge - 2,500 questions across mathematics, humanities, and natural sciences designed by subject-matter experts globally | `bench eval hle` |
| Humanity's Last Exam (Text-Only) | Text-only variant of HLE with multi-modal questions filtered out - evaluates models without vision capabilities on text-based questions from the frontier of human knowledge | `bench eval hle_text` |
| JSONSchemaBench | JSON Schema generation benchmark with ~10K real-world schemas from GitHub, Kubernetes, and other sources for evaluating constrained decoding | `bench eval jsonschemabench` |
| MATH | Measuring Mathematical Problem Solving - 5000 competition math problems across 7 subjects and 5 difficulty levels | `bench eval math` |
| MATH-500 | 500-problem subset of MATH dataset for faster evaluation of mathematical problem solving | `bench eval math_500` |
| MBPP | Mostly Basic Python Problems — code generation tasks with unit test verification | `bench eval mbpp` |
| MGSM | Multilingual Grade School Math benchmark across 11 languages for testing mathematical reasoning | `bench eval mgsm` |
| MGSM English | Grade school math problems in English for testing mathematical reasoning | `bench eval mgsm_en` |
| MGSM Latin Script | Grade school math problems in Latin script languages (German, English, Spanish, French, Swahili) | `bench eval mgsm_latin` |
| MGSM Non-Latin Script | Grade school math problems in non-Latin script languages (Bengali, Japanese, Russian, Telugu, Thai, Chinese) | `bench eval mgsm_non_latin` |
| MMLU (cais/mmlu) | Massive Multitask Language Understanding - 57 academic subjects from the cais/mmlu dataset | `bench eval mmlu` |
| MMLU Pro (TIGER-Lab) | Enhanced version of MMLU with more challenging, reasoning-focused questions | `bench eval mmlu-pro` |
| MMMU | Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark with 11.5K questions across 30 subjects from college exams, quizzes, and textbooks | `bench eval mmmu` |
| MMMU Accounting | MMMU Accounting subset focusing on accounting principles and practices | `bench eval mmmu_accounting` |
| MMMU Agriculture | MMMU Agriculture subset focusing on agricultural sciences and practices | `bench eval mmmu_agriculture` |
| MMMU Architecture and Engineering | MMMU Architecture and Engineering subset focusing on engineering design and architecture | `bench eval mmmu_architecture_and_engineering` |
| MMMU Art | MMMU Art subset focusing on art and visual design questions | `bench eval mmmu_art` |
| MMMU Art Theory | MMMU Art Theory subset focusing on art history and theoretical concepts | `bench eval mmmu_art_theory` |
| MMMU Basic Medical Science | MMMU Basic Medical Science subset focusing on fundamental medical knowledge | `bench eval mmmu_basic_medical_science` |
| MMMU Biology | MMMU Biology subset focusing on biological sciences | `bench eval mmmu_biology` |
| MMMU Chemistry | MMMU Chemistry subset focusing on chemical sciences | `bench eval mmmu_chemistry` |
| MMMU Clinical Medicine | MMMU Clinical Medicine subset focusing on clinical medical practice | `bench eval mmmu_clinical_medicine` |
| MMMU Design | MMMU Design subset focusing on design principles and practices | `bench eval mmmu_design` |
| MMMU Diagnostics and Laboratory Medicine | MMMU Diagnostics and Laboratory Medicine subset focusing on medical diagnostics | `bench eval mmmu_diagnostics_and_laboratory_medicine` |
| MMMU Electronics | MMMU Electronics subset focusing on electronic systems and circuits | `bench eval mmmu_electronics` |
| MMMU Energy and Power | MMMU Energy and Power subset focusing on energy systems and power generation | `bench eval mmmu_energy_and_power` |
| MMMU Finance | MMMU Finance subset focusing on financial concepts and analysis | `bench eval mmmu_finance` |
| MMMU Geography | MMMU Geography subset focusing on geographical knowledge and analysis | `bench eval mmmu_geography` |
| MMMU History | MMMU History subset focusing on historical knowledge and analysis | `bench eval mmmu_history` |
| MMMU Literature | MMMU Literature subset focusing on literary analysis and knowledge | `bench eval mmmu_literature` |
| MMMU Management | MMMU Management subset focusing on management principles and practices | `bench eval mmmu_manage` |
| MMMU Marketing | MMMU Marketing subset focusing on marketing strategies and concepts | `bench eval mmmu_marketing` |
| MMMU Materials | MMMU Materials subset focusing on materials science and engineering | `bench eval mmmu_materials` |
| MMMU Math | MMMU Mathematics subset focusing on mathematical reasoning | `bench eval mmmu_math` |
| MMMU MCQ | MMMU MCQ subset focusing on multiple choice questions | `bench eval mmmu_mcq` |
| MMMU Mechanical Engineering | MMMU Mechanical Engineering subset focusing on mechanical systems and design | `bench eval mmmu_mechanical_engineering` |
| MMMU Music | MMMU Music subset focusing on music theory and analysis | `bench eval mmmu_music` |
| MMMU Open | MMMU Open subset focusing on open-ended questions | `bench eval mmmu_open` |
| MMMU Pharmacy | MMMU Pharmacy subset focusing on pharmaceutical sciences and practice | `bench eval mmmu_pharmacy` |
| MMMU Physics | MMMU Physics subset focusing on physics and physical sciences | `bench eval mmmu_physics` |
| MMMU Public Health | MMMU Public Health subset focusing on public health concepts and practices | `bench eval mmmu_public_health` |
| MMMU Sociology | MMMU Sociology subset focusing on sociological concepts and analysis | `bench eval mmmu_sociology` |
| MMMU-Pro | Enhanced multimodal MMMU-Pro benchmark with multiple-choice across many options and images | `bench eval mmmu_pro` |
| MMMU-Pro (Vision) | MMMU-Pro vision subset with images and multiple-choice questions | `bench eval mmmu_pro_vision` |
| MuSR | Testing the Limits of Chain-of-thought with Multistep Soft Reasoning - includes murder mysteries, object placements, and team allocation tasks | `bench eval musr` |
| MuSR Murder Mysteries | MuSR murder mystery scenarios - who is the most likely murderer? | `bench eval musr_murder_mysteries` |
| MuSR Object Placements | MuSR object placement reasoning - where would someone look for an object? | `bench eval musr_object_placements` |
| MuSR Team Allocation | MuSR team allocation problems - how to allocate people to tasks efficiently? | `bench eval musr_team_allocation` |
| OpenAI MRCR (2 Needles) | Memory-Recall with Contextual Retrieval - long-context evaluation that measures recall of 2 needles across million-token contexts | `bench eval openai_mrcr_2n` |
| OpenAI MRCR (4 Needles) | Memory-Recall with Contextual Retrieval - long-context evaluation that measures recall of 4 needles across million-token contexts | `bench eval openai_mrcr_4n` |
| OpenAI MRCR (8 Needles) | Memory-Recall with Contextual Retrieval - long-context evaluation that measures recall of 8 needles across million-token contexts | `bench eval openai_mrcr_8n` |
| OpenAI MRCR (Full) | Memory-Recall with Contextual Retrieval - long-context evaluation that measures recall of 2, 4, and 8 needles across million-token contexts | `bench eval openai_mrcr` |
| OpenBookQA | Elementary-level science questions probing understanding of core facts | `bench eval openbookqa` |
| SciCode | Scientific computing and programming challenges | `bench eval scicode` |
| SimpleQA | Measuring short-form factuality in large language models with simple Q&A pairs | `bench eval simpleqa` |
| SuperGPQA | Scaling LLM Evaluation across 285 Graduate Disciplines - 26,529 multiple-choice questions across science, engineering, medicine, economics, and philosophy | `bench eval supergpqa` |
| TUMLU | TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages. | `bench eval tumlu`
