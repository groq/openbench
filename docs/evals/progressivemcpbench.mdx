---
title: ProgressiveMCPBench
description: Evaluating LLM agents with progressive tool discovery strategies using MCP
---

# ProgressiveMCPBench

ProgressiveMCPBench is a benchmark for evaluating how effectively language models can discover and use Model Context Protocol (MCP) tools. It tests agents on tasks that require using MCP tools - from file operations to API calls - while controlling *how* tools are presented to the model.

## Quick Start

```bash
# Run with directory-style tool discovery
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=directory

# Run with only required servers per task
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=minimal-servers

# Run with only exact required tools per task
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=minimal-tools

# Run with Groq server-side MCP (single-shot, no local tool loop)
bench eval progressivemcpbench --model groq-responses/openai/gpt-oss-20b -T strategy=minimal-servers-remote
```

The synthetic MCP HTTP server is started automatically when you run the eval (except for `minimal-servers-remote` which uses a hosted remote server).

## Architecture Overview

The benchmark uses a **synthetic MCP layer** that provides:
- **Deterministic responses** - Same inputs always produce same outputs
- **Fast execution** - No network latency or server startup time  
- **No dependencies** - No Node.js, npm packages, or external services required
- **Reliable evaluation** - No infrastructure failures or timeouts

```
┌─────────────────────────────────────────────────────────────┐
│                     Inspect AI Eval                         │
├─────────────────────────────────────────────────────────────┤
│  Strategy Layer                                             │
│  ├─ copilot ────────────► Semantic search discovery         │
│  ├─ directory ──────────► Filesystem-like exploration       │
│  ├─ minimal-servers ────► Server-level filtering            │
│  ├─ minimal-tools ──────► Exact tools only                  │
│  ├─ distraction-64 ─────► Required + distractors (64)       │
│  └─ distraction-128 ────► Required + distractors (128)      │
├─────────────────────────────────────────────────────────────┤
│  Synthetic HTTP MCP Server (localhost:8765)                 │
│  ├─ filesystem handler ──► data/files/root/                 │
│  ├─ table_lookup handler ► data/api/*.json                  │
│  ├─ excel_reader handler ► data/files/root/**/*.xlsx        │
│  ├─ static_json handler ─► Fixed responses                  │
│  └─ web_corpus handler ──► data/web/                        │
└─────────────────────────────────────────────────────────────┘
```

---

## Strategies

All strategies use the synthetic MCP layer with deterministic responses.

| Strategy                 | Description                                                 | Use Case                          |
|--------------------------|-------------------------------------------------------------|-----------------------------------|
| `copilot`                | Semantic search via `route()` + `execute-tool()`            | Tests RAG-style tool discovery    |
| `directory`              | Filesystem-like exploration via `ls()` + `read-tool-file()` | Tests explicit tool browsing      |
| `minimal-servers`        | Direct access to required servers only                      | Tests with server-level filtering |
| `minimal-servers-remote` | Groq server-side MCP (single-shot)                          | Tests remote MCP orchestration    |
| `minimal-tools`          | Direct access to exact tools needed                         | Upper bound on performance        |
| `distraction-64`         | Required tools + distractors (64 total)                     | Tests distraction resistance      |
| `distraction-128`        | Required tools + distractors (128 total)                    | Tests distraction resistance      |

### Strategy Details

**Copilot** - The model uses two meta-tools:
- `route(query)` - Semantic search across all available tools using embeddings
- `execute-tool(server, tool, params)` - Execute a discovered tool

**Directory** - Tools are presented as a filesystem:
- `ls(path)` - List available servers or tools within a server
- `read-tool-file(paths)` - Read tool descriptions (can batch multiple)
- `execute-tool(tool_path, params)` - Execute a tool by its path

**Minimal strategies** - No discovery needed; tools are provided directly based on task annotations. Requires `required_servers` and `required_tools` fields in tasks.

**Minimal-servers-remote** - Uses Groq's server-side MCP capability. Instead of local tool execution, MCP server specs are sent to Groq's Responses API, which handles all tool discovery and execution internally. This is a single-shot approach - no local tool-calling loop. Only works with the `groq-responses` provider.

```bash
# Example: Run with remote MCP
bench eval progressivemcpbench --model groq-responses/openai/gpt-oss-20b -T strategy=minimal-servers-remote
```

**Distraction strategies** - Required tools plus additional "distractor" tools, padded to exactly 64 or 128 tools total. Distractors are selected deterministically based on task ID.

---

## Adding New Tasks and Servers

This section explains how to add new tasks and MCP servers to the benchmark.

### Key Files

The benchmark uses these source files (the ones you edit):

| File | Purpose |
|------|---------|
| `synthetic_mcp/config/seeds/working_tasks.json` | Task definitions with questions, answers, and tool requirements |
| `synthetic_mcp/config/seeds/servers_raw.json` | Server definitions with tool schemas |
| `synthetic_mcp/data/api/*.json` | Data files for `table_lookup` and `static_json` handlers |
| `synthetic_mcp/data/files/root/` | Synthetic filesystem for `filesystem` handler |

And these generated files (regenerated by scripts):

| File | Generated By |
|------|--------------|
| `synthetic_mcp/config/servers.json` | `step1_generate_schemas.py` |
| `synthetic_mcp/tasks/progressivemcpbench.json` | `step3_generate_tasks.py` |

### Step-by-Step: Adding a New Server and Task

#### 1. Add the server to `servers_raw.json`

Edit `synthetic_mcp/config/seeds/servers_raw.json` and add your server definition:

```json
{
  "my-new-server": {
    "name": "My New Server",
    "description": "Description of what this server does",
    "category": "Finance",
    "web": "https://github.com/example/my-server",
    "in_clean_config": true,
    "tools": [
      {
        "name": "get_data",
        "description": "Get data by key",
        "inputSchema": {
          "type": "object",
          "properties": {
            "key": {
              "type": "string",
              "description": "The key to look up"
            }
          },
          "required": ["key"]
        },
        "annotations": null
      },
      {
        "name": "do_action",
        "description": "Perform an action (requires authentication)",
        "inputSchema": {
          "type": "object",
          "properties": {
            "action": {"type": "string"}
          },
          "required": ["action"]
        },
        "annotations": null
      }
    ]
  }
}
```

#### 2. Create data files (if needed)

For tools that return data, create JSON files in `synthetic_mcp/data/api/`:

```json
// synthetic_mcp/data/api/my_data.json
{
  "_comment": "Data for my-new-server",
  "items": {
    "key1": {"value": 100, "name": "First Item"},
    "key2": {"value": 200, "name": "Second Item"}
  }
}
```

#### 3. Add tasks to `working_tasks.json`

Edit `synthetic_mcp/config/seeds/working_tasks.json` and add your tasks:

```json
{
  "task_id": "unique-uuid-here",
  "Question": "What is the value of key1 from my-new-server?",
  "category": "Finance",
  "file_name": "",
  "answer": "100",
  "scorer_instructions": "Accept the numeric value 100 in any format.",
  "required_servers": ["my-new-server"],
  "required_tools": ["get_data"],
  "annotator_metadata": {
    "Number of steps": "1",
    "Number of tools": "1",
    "Steps": "1. Call get_data with key1",
    "Tools": "1. get_data"
  }
}
```

#### 4. Run `step1_generate_schemas.py`

This generates `servers.json` with handler configurations:

```bash
python synthetic_mcp/generation/step1_generate_schemas.py
```

The script will:
- Preserve existing handlers for tools that already have them
- Create stub handlers for new tools
- Attempt to use an LLM to generate handlers for required tools (if `GROQ_API_KEY` is set)

#### 5. Fix handlers in `servers.json` (if needed)

After step1, check `synthetic_mcp/config/servers.json` for your new server. You may need to manually update handlers from stubs to working implementations:

```json
{
  "name": "get_data",
  "handler": {
    "type": "table_lookup",
    "dataset": "data/api/my_data.json",
    "key_field": "key",
    "nested_path": "items"
  }
}
```

For stubbed actions that should return errors:

```json
{
  "name": "do_action",
  "handler": {
    "type": "static_json",
    "response": {
      "success": false,
      "error": "This action requires authentication."
    }
  }
}
```

#### 6. Run `step3_generate_tasks.py`

This generates the task file that the eval actually reads:

```bash
python synthetic_mcp/generation/step3_generate_tasks.py
```

This creates `synthetic_mcp/tasks/progressivemcpbench.json` from your `working_tasks.json`.

#### 7. Validate (optional)

```bash
python synthetic_mcp/generation/step4_validate_all.py
```

#### 8. Run the eval

```bash
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=minimal-servers --limit 5
```

### Handler Types Reference

| Handler        | Purpose                              | Data Location               |
|----------------|--------------------------------------|-----------------------------|
| `filesystem`   | Read files from synthetic filesystem | `data/files/root/`          |
| `table_lookup` | Query JSON data by key               | `data/api/*.json`           |
| `table_search` | Search JSON data with decoys         | `data/api/*.json`           |
| `excel_reader` | Read Excel files                     | `data/files/root/**/*.xlsx` |
| `static_json`  | Return fixed JSON responses          | Inline in handler           |
| `web_corpus`   | Fetch synthetic web pages            | `data/web/`                 |
| `hackernews_story` | HackerNews story lookup          | `data/api/hackernews_stories.json` |
| `wikipedia_search` | Wikipedia article search         | `data/api/wikipedia_articles.json` |

### Example: Commodity Price + Forex Task

Here's a real example that requires two servers and multi-step reasoning:

**Servers** (in `servers_raw.json`):
- `commodities-markets`: Provides `get_commodity_price` returning USD/troy oz
- `forex`: Provides `get_exchange_rate` for currency conversion

**Data files**:
- `data/api/commodity_prices.json`: Gold at $2650/oz, Silver at $31.50/oz
- `data/api/forex_rates.json`: USD/AUD rate of 1.58

**Task** (in `working_tasks.json`):
```json
{
  "task_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "Question": "I have 18.9g silver, what is it worth in AUD as of today?",
  "answer": "30.25",
  "scorer_instructions": "Accept answers between 29.50 and 31.00 AUD.",
  "required_servers": ["commodities-markets", "forex"],
  "required_tools": ["get_commodity_price", "get_exchange_rate"]
}
```

The model must:
1. Know that precious metals are priced per troy ounce (31.1035g)
2. Convert 18.9g to troy ounces
3. Look up silver price in USD
4. Look up USD/AUD exchange rate
5. Calculate the final value

---

## Generation Pipeline

The complete pipeline for regenerating everything from scratch:

```bash
# Step 0: Extract seeds from the original progressivemcpbench.json
# (Only needed if starting from scratch or syncing with original dataset)
python synthetic_mcp/generation/step0_extract_seeds.py

# Step 1: Generate servers.json with handlers
python synthetic_mcp/generation/step1_generate_schemas.py

# Step 2: Generate test data files (optional, for new data)
python synthetic_mcp/generation/step2_generate_data.py

# Step 3: Generate the task file used by the eval
python synthetic_mcp/generation/step3_generate_tasks.py

# Step 4: Validate all tasks work
python synthetic_mcp/generation/step4_validate_all.py

# Step 5: Apply handler overrides from stub annotations
python synthetic_mcp/generation/step5_apply_annotations.py
```

**Most common workflow** (adding new tasks/servers):
```bash
# After editing working_tasks.json and servers_raw.json:
python synthetic_mcp/generation/step1_generate_schemas.py
# Fix any handlers in servers.json if needed
python synthetic_mcp/generation/step3_generate_tasks.py
```

### Stub Call Logging

When running evaluations, the HTTP server logs calls to stub handlers (tools without real implementations) to `synthetic_mcp/logs/stub_calls.json`. This enables iterative development:

1. Run evaluation with `minimal-servers` strategy
2. Review `stub_calls.json` to see which tools were called
3. Annotate entries with `handler_override` for what the tool should return
4. Run `step5_apply_annotations.py` to apply overrides
5. Re-run evaluation

---

## Directory Structure

```
synthetic_mcp/
├── config/
│   ├── seeds/                    # SOURCE FILES (edit these)
│   │   ├── servers_raw.json      # Server definitions
│   │   └── working_tasks.json    # Task definitions
│   └── servers.json              # GENERATED: Tool schemas + handlers
├── data/
│   ├── files/root/               # Synthetic filesystem
│   │   ├── csv/
│   │   ├── excel/
│   │   ├── music/
│   │   ├── pdf/
│   │   ├── txt/
│   │   └── word/
│   ├── api/                      # JSON lookup tables
│   │   ├── commodity_prices.json
│   │   ├── forex_rates.json
│   │   ├── trials.json
│   │   └── ...
│   └── web/                      # Synthetic web corpus
├── tasks/
│   └── progressivemcpbench.json            # GENERATED: What the eval reads
├── logs/
│   └── stub_calls.json           # Logged stub handler calls (gitignored)
├── server/
│   └── http_mcp_server.py        # The synthetic HTTP server
└── generation/                   # Pipeline scripts
    ├── step0_extract_seeds.py
    ├── step1_generate_schemas.py
    ├── step2_generate_data.py
    ├── step3_generate_tasks.py
    ├── step4_validate_all.py
    └── step5_apply_annotations.py
```

---

## Scoring

Tasks are scored using LLM-as-a-judge with the SimpleQA grader template:

| Score | Meaning                         |
|-------|---------------------------------|
| 1.0   | Correct answer                  |
| 0.5   | Partially correct (fuzzy match) |
| 0.0   | Incorrect or no answer          |

The model must output a JSON object with a `final_answer` field containing the answer.

---

## CLI Reference

```bash
bench eval progressivemcpbench --model <model> -T strategy=<strategy> [options]

# Strategy options:
-T strategy=copilot                 # Semantic search discovery
-T strategy=directory               # Filesystem-like exploration
-T strategy=minimal-servers         # Only required servers
-T strategy=minimal-servers-remote  # Groq server-side MCP (groq-responses only)
-T strategy=minimal-tools           # Only exact tools
-T strategy=distraction-64          # Required + distractors (64 total)
-T strategy=distraction-128         # Required + distractors (128 total)

# Common options:
--limit N          # Run only N tasks
--epochs N         # Run each task N times  
--epochs-reducer   # How to combine epoch scores (mean, max, etc.)
--log-dir DIR      # Directory for eval logs
```

---

## Requirements

- Python 3.10+
- OPENAI_API_KEY (for embeddings in copilot strategy)

---

## References

Based on [LiveMCPBench](https://github.com/icip-cas/LiveMCPBench) with extensions for progressive tool discovery research.
