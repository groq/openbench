---
title: ProgressiveMCPBench
description: Evaluating LLM agents with progressive tool discovery strategies using MCP
---

# ProgressiveMCPBench

ProgressiveMCPBench is a benchmark for evaluating how effectively language models can discover and use Model Context Protocol (MCP) tools. It tests agents on tasks that require using MCP tools - from file operations to API calls - while controlling *how* tools are presented to the model.

## Quick Start

```bash
# Start the HTTP MCP server (in a separate terminal)
python synthetic_mcp/server/http_mcp_server.py

# Run with all synthetic tools
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=synthetic

# Run with only required servers per task
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=synthetic-minimal-servers

# Run with only exact required tools per task
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=synthetic-minimal-tools
```

## Architecture Overview

The benchmark uses a **synthetic MCP layer** that provides:
- **Deterministic responses** - Same inputs always produce same outputs
- **Fast execution** - No network latency or server startup time  
- **No dependencies** - No Node.js, npm packages, or external services required
- **Reliable evaluation** - No infrastructure failures or timeouts

```
┌─────────────────────────────────────────────────────────────┐
│                     Inspect AI Eval                         │
├─────────────────────────────────────────────────────────────┤
│  Strategy Layer                                             │
│  ├─ synthetic ──────────► All synthetic tools               │
│  ├─ synthetic-minimal-servers ► Server-level filtering      │
│  └─ synthetic-minimal-tools ──► Exact tools only            │
├─────────────────────────────────────────────────────────────┤
│  Synthetic HTTP MCP Server (localhost:8765)                 │
│  ├─ filesystem handler ──► data/files/root/                 │
│  ├─ table_lookup handler ► data/api/*.json                  │
│  ├─ excel_reader handler ► data/files/root/**/*.xlsx        │
│  ├─ static_json handler ─► Fixed responses                  │
│  └─ compute handler ─────► Formula evaluation               │
└─────────────────────────────────────────────────────────────┘
```

---

## Strategies

### Synthetic Strategies (Recommended)

| Strategy | Description | Use Case |
|----------|-------------|----------|
| `synthetic` | All synthetic tools available | Baseline with full tool set |
| `synthetic-minimal-servers` | Only required servers per task | Tests with server-level filtering |
| `synthetic-minimal-tools` | Only exact tools needed | Upper bound on performance |

### Live MCP Strategies

These strategies use real MCP servers (requires Node.js and network access):

| Strategy | Description |
|----------|-------------|
| `copilot` | Semantic search via `route()` + `execute-tool()` |
| `directory` | Filesystem-like exploration via `ls()` + `read-tool-file()` |
| `minimal-servers` | Direct access to required servers only |
| `minimal-tools` | Direct access to exact tools needed |
| `distraction-64` | Required tools + distractors (64 total) |
| `distraction-128` | Required tools + distractors (128 total) |

---

## Expanding the Task Set

The benchmark currently uses a curated set of tasks with verified answers. This section explains how to expand coverage.

### Current State

- **Working tasks:** 13 tasks in `synthetic_mcp/config/seeds/working_tasks.json`
- **Original dataset:** ~85 tasks in `src/openbench/datasets/data/progressivemcpbench.json` (most lack answers)
- **Synthetic servers:** 11 servers defined in `synthetic_mcp/config/servers.json`

### Expansion Approaches

There are three ways to expand the task set:

1. **Port more tasks from the original dataset** - Convert tasks that have or can have deterministic answers
2. **Generate new tasks programmatically** - Create tasks from existing synthetic data  
3. **Add new MCP servers** - Introduce new tool domains with backing data

### Approach 1: Converting Original Tasks

The original `progressivemcpbench.json` contains many tasks that can be converted to synthetic tasks.

#### Step 1: Classify Original Tasks

Tasks fall into categories:

| Category | Characteristics | Action |
|----------|-----------------|--------|
| **A: Ready to use** | Has `answer`, has `required_servers`, not time-dependent | Add directly |
| **B: Has `_answer`** | Flagged as flaky but has known answer | May need data freeze |
| **C: No answer, but deterministic** | E.g., stock analysis, paper lookup | Create synthetic data |
| **D: Inherently dynamic** | "today's trending", live web automation | Convert to scalar Q&A or skip |

#### Step 2: Add Tasks to Seeds

Create `synthetic_mcp/config/seeds/extra_working_tasks.json`:

```json
[
  {
    "task_id": "unique-id-here",
    "Question": "What is the closing price of AAPL on 2024-06-15?",
    "category": "Finance",
    "file_name": "/root/csv/apple_prices_2024.csv",
    "answer": "195.87",
    "scorer_instructions": "Exact match required",
    "required_servers": ["filesystem"],
    "required_tools": ["read_file"],
    "annotator_metadata": {}
  }
]
```

#### Step 3: Regenerate Tasks

```bash
# Regenerate (step0 pulls from progressivemcpbench.json)
python synthetic_mcp/generation/step3_generate_tasks.py

# Validate
python synthetic_mcp/generation/step4_validate_all.py
```

### Approach 2: Programmatic Task Generation

Generate many tasks from existing synthetic data using templates.

#### Task Templates by Handler Type

**Filesystem tasks:**
```python
# Template: "Get line N of file X"
# Template: "From file X, what is the [field] for [entity]?"
# Template: "In BibTeX file X, what is the title of the first paper?"
```

**Excel tasks:**
```python
# Template: "Read sheet S from workbook W and return [derived scalar]"
# Example: "What is Alice's BMI from people_data.xlsx?"
```

**Table lookup tasks:**
```python
# Template: "From dataset D, which row satisfies [predicate]?"
# Example: "Which clinical trial has NCT ID NCT04280705?"
```

#### Creating a Task Generator

Add `synthetic_mcp/generation/step3b_generate_tasks.py`:

```python
def generate_csv_lookup_tasks(csv_path: Path, limit: int = 5) -> list[dict]:
    """Generate tasks that query rows from a CSV file."""
    import csv
    tasks = []
    
    with open(csv_path) as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    for i, row in enumerate(random.sample(rows, min(limit, len(rows)))):
        # Pick a random field to query
        key_field = "Name"  # or detect from schema
        value_field = random.choice([f for f in row.keys() if f != key_field])
        
        tasks.append({
            "task_id": str(uuid.uuid4()),
            "Question": f"From {csv_path.name}, what is the {value_field} for {row[key_field]}?",
            "answer": row[value_field],
            "required_servers": ["filesystem"],
            "required_tools": ["read_file"],
            # ...
        })
    
    return tasks
```

### Approach 3: Adding New MCP Servers

To add a new synthetic MCP server:

#### Step 1: Define the Server in servers.json

Add to `synthetic_mcp/config/servers.json`:

```json
{
  "travel": {
    "name": "Travel Planning Server",
    "description": "Static travel data for benchmarking",
    "tools": [
      {
        "name": "get_route_directions",
        "description": "Get directions between two locations",
        "handler": {
          "type": "table_lookup",
          "table": "travel/routes.json",
          "key_fields": ["origin", "destination"]
        },
        "inputSchema": {
          "type": "object",
          "properties": {
            "origin": {"type": "string"},
            "destination": {"type": "string"}
          },
          "required": ["origin", "destination"]
        }
      }
    ]
  }
}
```

#### Step 2: Create Backing Data

Add data files under `synthetic_mcp/data/`:

```json
// synthetic_mcp/data/api/travel/routes.json
[
  {
    "origin": "Beijing",
    "destination": "Shanghai", 
    "distance_km": 1318,
    "duration_hours": 4.5,
    "mode": "high-speed-rail"
  }
]
```

#### Step 3: Add Tasks Using the New Server

```json
{
  "task_id": "travel-task-001",
  "Question": "How long does it take to travel from Beijing to Shanghai by high-speed rail?",
  "answer": "4.5 hours",
  "required_servers": ["travel"],
  "required_tools": ["get_route_directions"]
}
```

#### Step 4: Validate

```bash
python synthetic_mcp/generation/step4_validate_all.py
```

### Handler Types Reference

| Handler | Purpose | Data Location |
|---------|---------|---------------|
| `filesystem` | Read files from synthetic filesystem | `data/files/root/` |
| `table_lookup` | Query JSON tables | `data/api/` |
| `excel_reader` | Read Excel files | `data/files/root/**/*.xlsx` |
| `static_json` | Return fixed JSON responses | Inline in handler |
| `compute` | Evaluate formulas | Expression in handler |

---

## Tasks vs Epochs

When configuring evaluations, consider the trade-off between task count and epochs:

| Configuration | Tasks | Epochs | Total Runs | Use Case |
|---------------|-------|--------|------------|----------|
| Quick smoke test | 20 | 2 | 40 | Development iteration |
| Standard eval | 50-80 | 3-5 | 150-400 | Regular benchmarking |
| Full eval | 100+ | 3 | 300+ | Publication-quality |

**Recommendation:** Favor more unique tasks with 3-5 epochs over fewer tasks with many epochs. Diversity of tools and reasoning patterns is more valuable than reducing variance on a small set.

---

## Generation Pipeline

The synthetic data pipeline lives in `synthetic_mcp/generation/`:

```bash
# Step 0: Extract working tasks from progressivemcpbench.json
python synthetic_mcp/generation/step0_extract_seeds.py

# Step 1: Generate tool schemas with handlers (requires GROQ_API_KEY)
python synthetic_mcp/generation/step1_generate_schemas.py

# Step 2: Generate test data files
python synthetic_mcp/generation/step2_generate_data.py

# Step 3: Generate task list from seeds
python synthetic_mcp/generation/step3_generate_tasks.py

# Step 4: Validate all tasks work with the HTTP server
python synthetic_mcp/generation/step4_validate_all.py
```

### Directory Structure

```
synthetic_mcp/
├── config/
│   ├── seeds/              # Step 0 outputs
│   │   ├── servers_raw.json
│   │   └── working_tasks.json
│   └── servers.json        # Tool schemas + handlers
├── data/
│   ├── files/root/         # Synthetic filesystem
│   │   ├── csv/
│   │   ├── excel/
│   │   ├── music/
│   │   ├── pdf/
│   │   ├── txt/
│   │   └── word/
│   └── api/                # JSON lookup tables
├── tasks/
│   └── progressivemcpbench_synthetic.json
├── server/
│   └── http_mcp_server.py
└── generation/             # Pipeline scripts
```

---

## Scoring

Tasks are scored using LLM-as-a-judge with the SimpleQA grader template:

| Score | Meaning |
|-------|---------|
| 1.0 | Correct answer |
| 0.5 | Partially correct (fuzzy match) |
| 0.0 | Incorrect or no answer |

The model must output a JSON object with a `final_answer` field containing the answer.

---

## CLI Reference

```bash
bench eval progressivemcpbench --model <model> -T strategy=<strategy> [options]

# Strategy options:
-T strategy=synthetic                  # All synthetic tools
-T strategy=synthetic-minimal-servers  # Only required servers
-T strategy=synthetic-minimal-tools    # Only exact tools

# Common options:
--limit N          # Run only N tasks
--epochs N         # Run each task N times  
--epochs-reducer   # How to combine epoch scores (mean, max, etc.)
--log-dir DIR      # Directory for eval logs
```

---

## Requirements

**For synthetic strategies:**
- Python 3.10+
- The HTTP MCP server running (`python synthetic_mcp/server/http_mcp_server.py`)

**For live MCP strategies:**
- OPENAI_API_KEY (for embeddings in copilot strategy)
- Node.js (for running MCP servers)
- Various MCP server dependencies (automatically managed)

---

## References

Based on [LiveMCPBench](https://github.com/icip-cas/LiveMCPBench) with extensions for progressive tool discovery research.
