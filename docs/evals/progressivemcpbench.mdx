---
title: ProgressiveMCPBench
description: Evaluating LLM agents with progressive tool discovery strategies using MCP
---

# ProgressiveMCPBench

ProgressiveMCPBench is a benchmark for evaluating how effectively language models can discover and use Model Context Protocol (MCP) tools. It tests agents on tasks that require using MCP tools - from file operations to API calls - while controlling *how* tools are presented to the model.

## Quick Start

```bash
# Run with directory-style tool discovery
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=directory

# Run with only required servers per task
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=minimal-servers

# Run with only exact required tools per task
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=minimal-tools
```

The synthetic MCP HTTP server is started automatically when you run the eval.

## Architecture Overview

The benchmark uses a **synthetic MCP layer** that provides:
- **Deterministic responses** - Same inputs always produce same outputs
- **Fast execution** - No network latency or server startup time  
- **No dependencies** - No Node.js, npm packages, or external services required
- **Reliable evaluation** - No infrastructure failures or timeouts

```
┌─────────────────────────────────────────────────────────────┐
│                     Inspect AI Eval                         │
├─────────────────────────────────────────────────────────────┤
│  Strategy Layer                                             │
│  ├─ copilot ────────────► Semantic search discovery         │
│  ├─ directory ──────────► Filesystem-like exploration       │
│  ├─ minimal-servers ────► Server-level filtering            │
│  ├─ minimal-tools ──────► Exact tools only                  │
│  ├─ distraction-64 ─────► Required + distractors (64)       │
│  └─ distraction-128 ────► Required + distractors (128)      │
├─────────────────────────────────────────────────────────────┤
│  Synthetic HTTP MCP Server (localhost:8765)                 │
│  ├─ filesystem handler ──► data/files/root/                 │
│  ├─ table_lookup handler ► data/api/*.json                  │
│  ├─ excel_reader handler ► data/files/root/**/*.xlsx        │
│  ├─ static_json handler ─► Fixed responses                  │
│  └─ compute handler ─────► Formula evaluation               │
└─────────────────────────────────────────────────────────────┘
```

---

## Strategies

All strategies use the synthetic MCP layer with deterministic responses.

| Strategy          | Description                                                 | Use Case                          |
|-------------------|-------------------------------------------------------------|-----------------------------------|
| `copilot`         | Semantic search via `route()` + `execute-tool()`            | Tests RAG-style tool discovery    |
| `directory`       | Filesystem-like exploration via `ls()` + `read-tool-file()` | Tests explicit tool browsing      |
| `minimal-servers` | Direct access to required servers only                      | Tests with server-level filtering |
| `minimal-tools`   | Direct access to exact tools needed                         | Upper bound on performance        |
| `distraction-64`  | Required tools + distractors (64 total)                     | Tests distraction resistance      |
| `distraction-128` | Required tools + distractors (128 total)                    | Tests distraction resistance      |

### Strategy Details

**Copilot** - The model uses two meta-tools:
- `route(query)` - Semantic search across all available tools using embeddings
- `execute-tool(server, tool, params)` - Execute a discovered tool

**Directory** - Tools are presented as a filesystem:
- `ls(path)` - List available servers or tools within a server
- `read-tool-file(paths)` - Read tool descriptions (can batch multiple)
- `execute-tool(tool_path, params)` - Execute a tool by its path

**Minimal strategies** - No discovery needed; tools are provided directly based on task annotations. Requires `required_servers` and `required_tools` fields in tasks.

**Distraction strategies** - Required tools plus additional "distractor" tools, padded to exactly 64 or 128 tools total. Distractors are selected deterministically based on task ID.

---

## Expanding the Task Set

The benchmark currently uses a curated set of tasks with verified answers. This section explains how to expand coverage.

### Current State

- **Working tasks:** 13 tasks in `synthetic_mcp/config/seeds/working_tasks.json`
- **Original dataset:** ~85 tasks in `src/openbench/datasets/data/progressivemcpbench.json` (most lack answers)
- **Synthetic servers:** 11 servers defined in `synthetic_mcp/config/servers.json`

### Expansion Approaches

There are three ways to expand the task set:

1. **Port more tasks from the original dataset** - Convert tasks that have or can have deterministic answers
2. **Generate new tasks programmatically** - Create tasks from existing synthetic data  
3. **Add new MCP servers** - Introduce new tool domains with backing data

### Approach 1: Converting Original Tasks

The original `progressivemcpbench.json` contains many tasks that can be converted to synthetic tasks.

#### Step 1: Classify Original Tasks

Tasks fall into categories:

| Category                            | Characteristics                                          | Action                        |
|-------------------------------------|----------------------------------------------------------|-------------------------------|
| **A: Ready to use**                 | Has `answer`, has `required_servers`, not time-dependent | Add directly                  |
| **B: Has `_answer`**                | Flagged as flaky but has known answer                    | May need data freeze          |
| **C: No answer, but deterministic** | E.g., stock analysis, paper lookup                       | Create synthetic data         |
| **D: Inherently dynamic**           | "today's trending", live web automation                  | Convert to scalar Q&A or skip |

#### Step 2: Add Tasks to Seeds

Create `synthetic_mcp/config/seeds/extra_working_tasks.json`:

```json
[
  {
    "task_id": "unique-id-here",
    "Question": "What is the closing price of AAPL on 2024-06-15?",
    "category": "Finance",
    "file_name": "/root/csv/apple_prices_2024.csv",
    "answer": "195.87",
    "scorer_instructions": "Exact match required",
    "required_servers": ["filesystem"],
    "required_tools": ["read_file"],
    "annotator_metadata": {}
  }
]
```

#### Step 3: Regenerate Tasks

```bash
# Regenerate (step0 pulls from progressivemcpbench.json)
python synthetic_mcp/generation/step3_generate_tasks.py

# Validate
python synthetic_mcp/generation/step4_validate_all.py
```

### Approach 2: Programmatic Task Generation

Generate many tasks from existing synthetic data using templates.

#### Task Templates by Handler Type

**Filesystem tasks:**
```python
# Template: "Get line N of file X"
# Template: "From file X, what is the [field] for [entity]?"
# Template: "In BibTeX file X, what is the title of the first paper?"
```

**Excel tasks:**
```python
# Template: "Read sheet S from workbook W and return [derived scalar]"
# Example: "What is Alice's BMI from people_data.xlsx?"
```

**Table lookup tasks:**
```python
# Template: "From dataset D, which row satisfies [predicate]?"
# Example: "Which clinical trial has NCT ID NCT04280705?"
```

#### Creating a Task Generator

Add `synthetic_mcp/generation/step3b_generate_tasks.py`:

```python
def generate_csv_lookup_tasks(csv_path: Path, limit: int = 5) -> list[dict]:
    """Generate tasks that query rows from a CSV file."""
    import csv
    tasks = []
    
    with open(csv_path) as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    for i, row in enumerate(random.sample(rows, min(limit, len(rows)))):
        # Pick a random field to query
        key_field = "Name"  # or detect from schema
        value_field = random.choice([f for f in row.keys() if f != key_field])
        
        tasks.append({
            "task_id": str(uuid.uuid4()),
            "Question": f"From {csv_path.name}, what is the {value_field} for {row[key_field]}?",
            "answer": row[value_field],
            "required_servers": ["filesystem"],
            "required_tools": ["read_file"],
            # ...
        })
    
    return tasks
```

### Approach 3: Adding New MCP Servers

To add a new synthetic MCP server:

#### Step 1: Define the Server in servers.json

Add to `synthetic_mcp/config/servers.json`:

```json
{
  "travel": {
    "name": "Travel Planning Server",
    "description": "Static travel data for benchmarking",
    "tools": [
      {
        "name": "get_route_directions",
        "description": "Get directions between two locations",
        "handler": {
          "type": "table_lookup",
          "table": "travel/routes.json",
          "key_fields": ["origin", "destination"]
        },
        "inputSchema": {
          "type": "object",
          "properties": {
            "origin": {"type": "string"},
            "destination": {"type": "string"}
          },
          "required": ["origin", "destination"]
        }
      }
    ]
  }
}
```

#### Step 2: Create Backing Data

Add data files under `synthetic_mcp/data/`:

```json
// synthetic_mcp/data/api/travel/routes.json
[
  {
    "origin": "Beijing",
    "destination": "Shanghai", 
    "distance_km": 1318,
    "duration_hours": 4.5,
    "mode": "high-speed-rail"
  }
]
```

#### Step 3: Add Tasks Using the New Server

```json
{
  "task_id": "travel-task-001",
  "Question": "How long does it take to travel from Beijing to Shanghai by high-speed rail?",
  "answer": "4.5 hours",
  "required_servers": ["travel"],
  "required_tools": ["get_route_directions"]
}
```

#### Step 4: Validate

```bash
python synthetic_mcp/generation/step4_validate_all.py
```

### Handler Types Reference

| Handler        | Purpose                              | Data Location               |
|----------------|--------------------------------------|-----------------------------|
| `filesystem`   | Read files from synthetic filesystem | `data/files/root/`          |
| `table_lookup` | Query JSON tables                    | `data/api/`                 |
| `excel_reader` | Read Excel files                     | `data/files/root/**/*.xlsx` |
| `static_json`  | Return fixed JSON responses          | Inline in handler           |
| `compute`      | Evaluate formulas                    | Expression in handler       |

---

## Tasks vs Epochs

When configuring evaluations, consider the trade-off between task count and epochs:

| Configuration    | Tasks | Epochs | Total Runs | Use Case              |
|------------------|-------|--------|------------|-----------------------|
| Quick smoke test | 20    | 2      | 40         | Development iteration |
| Standard eval    | 50-80 | 3-5    | 150-400    | Regular benchmarking  |
| Full eval        | 100+  | 3      | 300+       | Publication-quality   |

**Recommendation:** Favor more unique tasks with 3-5 epochs over fewer tasks with many epochs. Diversity of tools and reasoning patterns is more valuable than reducing variance on a small set.

---

## Generation Pipeline

The synthetic data pipeline lives in `synthetic_mcp/generation/`:

```bash
# Step 0: Extract working tasks from progressivemcpbench.json
python synthetic_mcp/generation/step0_extract_seeds.py

# Step 1: Generate tool schemas with handlers (requires GROQ_API_KEY)
python synthetic_mcp/generation/step1_generate_schemas.py

# Step 2: Generate test data files
python synthetic_mcp/generation/step2_generate_data.py

# Step 3: Generate task list from seeds
python synthetic_mcp/generation/step3_generate_tasks.py

# Step 4: Validate all tasks work with the HTTP server
python synthetic_mcp/generation/step4_validate_all.py
```

### Directory Structure

```
synthetic_mcp/
├── config/
│   ├── seeds/              # Step 0 outputs
│   │   ├── servers_raw.json
│   │   └── working_tasks.json
│   └── servers.json        # Tool schemas + handlers
├── data/
│   ├── files/root/         # Synthetic filesystem
│   │   ├── csv/
│   │   ├── excel/
│   │   ├── music/
│   │   ├── pdf/
│   │   ├── txt/
│   │   └── word/
│   └── api/                # JSON lookup tables
├── tasks/
│   └── progressivemcpbench_synthetic.json
├── server/
│   └── http_mcp_server.py
└── generation/             # Pipeline scripts
```

---

## Scoring

Tasks are scored using LLM-as-a-judge with the SimpleQA grader template:

| Score | Meaning                         |
|-------|---------------------------------|
| 1.0   | Correct answer                  |
| 0.5   | Partially correct (fuzzy match) |
| 0.0   | Incorrect or no answer          |

The model must output a JSON object with a `final_answer` field containing the answer.

---

## CLI Reference

```bash
bench eval progressivemcpbench --model <model> -T strategy=<strategy> [options]

# Strategy options:
-T strategy=copilot           # Semantic search discovery
-T strategy=directory         # Filesystem-like exploration
-T strategy=minimal-servers   # Only required servers
-T strategy=minimal-tools     # Only exact tools
-T strategy=distraction-64    # Required + distractors (64 total)
-T strategy=distraction-128   # Required + distractors (128 total)

# Common options:
--limit N          # Run only N tasks
--epochs N         # Run each task N times  
--epochs-reducer   # How to combine epoch scores (mean, max, etc.)
--log-dir DIR      # Directory for eval logs
```

---

## Requirements

- Python 3.10+
- OPENAI_API_KEY (for embeddings in copilot strategy)

---

## References

Based on [LiveMCPBench](https://github.com/icip-cas/LiveMCPBench) with extensions for progressive tool discovery research.
