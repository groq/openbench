---
title: ProgressiveMCPBench
description: Evaluating LLM agents with progressive tool discovery strategies using MCP
---

# ProgressiveMCPBench

<Callout type="warning">
**Work in Progress**: This benchmark is being migrated to a fully synthetic MCP layer. The documentation below describes the current (live MCP) implementation. See the [Synthetic MCP Pipeline](#synthetic-mcp-pipeline) section for regeneration instructions.
</Callout>

## Synthetic MCP Pipeline

We are developing a synthetic version of ProgressiveMCPBench that replaces live MCP servers with a single HTTP server serving deterministic responses. This eliminates infrastructure failures and enables faster, more reliable evaluation.

### Regenerating the Synthetic Data

The generation pipeline lives in `synthetic_mcp/generation/`. Run these steps in order:

```bash
# Ensure you're in the repo root with venv activated
cd /path/to/openbench
source .venv/bin/activate

# Stage 0: Extract seeds from working tasks
# - Reads progressivemcpbench.json and LiveMCPBench tools.json
# - Outputs: synthetic_mcp/config/seeds/servers_raw.json, working_tasks.json
python synthetic_mcp/generation/step0_extract_seeds.py

# Stage 1: Generate tool schemas with handlers
# - Uses gpt-oss-120b to generate handler specifications
# - Outputs: synthetic_mcp/config/servers.json
# - REVIEW THIS FILE before proceeding - manual refinement may be needed
python synthetic_mcp/generation/step1_generate_schemas.py

# Stage 2: Generate test data
# - Copies files from LiveMCPBench annotated_data
# - Generates JSON lookup tables for API-style tools
# - Outputs: synthetic_mcp/data/files/root/, synthetic_mcp/data/api/
python synthetic_mcp/generation/step2_generate_data.py

# Stage 3: Validate tasks from working_tasks.json
# - Uses the 13 working tasks from LiveMCPBench directly
# - Validates all required servers/tools exist in servers.json
# - Outputs: synthetic_mcp/tasks/progressivemcpbench_synthetic.json
python synthetic_mcp/generation/step3_generate_tasks.py

# Stage 4: Integration tests
# - Starts the HTTP MCP server
# - Runs tests against each tool handler
# - Validates expected answers match tool outputs
python synthetic_mcp/generation/step4_validate_all.py
```

### Environment Requirements

```bash
export GROQ_API_KEY=...  # Required for LLM generation steps
```

### Directory Structure

```
synthetic_mcp/
├── config/
│   ├── seeds/           # Stage 0 outputs (server/task extraction)
│   │   ├── servers_raw.json
│   │   └── working_tasks.json
│   └── servers.json     # Stage 1 output (tool schemas + handlers)
├── data/
│   ├── files/root/      # Synthetic filesystem (txt, csv, pdf, docx, music, excel)
│   │   ├── txt/         # Android.txt, log files, paper_list.bib
│   │   ├── csv/         # customers-100.csv
│   │   ├── excel/       # people_data.xlsx
│   │   ├── word/        # exchange.docx
│   │   ├── pdf/         # WebArena.pdf, embodied_ai_papers/
│   │   └── music/       # mixkit-retro-game-emergency-alarm-1000.wav
│   └── api/             # JSON lookup tables
│       ├── trials.json           # Clinical trials (biomcp)
│       ├── arxiv_papers.json     # arXiv papers (mcp-simple-arxiv)
│       ├── maven_versions.json   # Maven versions (maven-deps-server)
│       └── audio_files.json      # Audio metadata (music-analysis)
├── tasks/
│   ├── progressivemcpbench_synthetic.json  # The 13 working tasks
│   └── summary.json                        # Task statistics
├── server/
│   └── http_mcp_server.py  # HTTP server masquerading as multiple MCP servers
└── generation/             # Pipeline scripts (step0-4)
```

---

ProgressiveMCPBench is a benchmark for evaluating how effectively language models can discover and use Model Context Protocol (MCP) tools. It extends LiveMCPBench by supporting multiple **strategies** for presenting tools to the model, allowing researchers to measure the impact of different tool discovery mechanisms on agent performance.

## Overview

The benchmark tests agents on real-world tasks that require using MCP tools - from file operations to API calls. The key innovation is that you can control *how* tools are presented to the model, ranging from semantic search to filesystem-like exploration to direct tool access.

## Strategies

ProgressiveMCPBench supports five strategies, each representing a different approach to tool discovery:

### 1. Copilot (Semantic Search)

```bash
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=copilot
```

The model uses two meta-tools:
- `route(query)` - Semantic search across all available tools using embeddings
- `execute-tool(server, tool, params)` - Execute a discovered tool

This simulates an AI assistant with a large library of tools that uses RAG-style retrieval.

### 2. Directory (Filesystem Exploration)

```bash
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=directory
```

Tools are presented as a directory structure:
- `ls(path)` - List available servers or tools within a server
- `read-tool-file(paths)` - Read tool descriptions (can batch multiple)
- `execute-tool(tool_path, params)` - Execute a tool by its path

Example exploration:
```
ls("/tools")              → "filesystem/\nweather/\ncalendar/..."
ls("/tools/filesystem")   → "read_file.md\nwrite_file.md\n..."
read-tool-file("/tools/filesystem/read_file.md")
execute-tool("/tools/filesystem/read_file.md", {"path": "/root/data.txt"})
```

### 3. Minimal Servers (Direct Server Access)

```bash
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=minimal-servers
```

The model receives all tools from only the server(s) required for each task. No discovery needed - tools are provided directly based on task annotations.

**Requires annotations:** The dataset must include `required_servers` for each task.

### 4. Minimal Tools (Exact Tool Access)

```bash
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=minimal-tools
```

The model receives only the exact tools needed for each task. This represents the ideal scenario where tool selection is perfect.

**Requires annotations:** The dataset must include `required_servers` and `required_tools` for each task.

### 5. Distraction-128 (Noise Testing)

```bash
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=distraction-128
```

The model receives the required tools plus additional "distractor" tools, padded to exactly 128 tools total. Distractors are selected deterministically based on the task ID, ensuring reproducibility.

**Requires annotations:** The dataset must include `required_servers` for each task.

## Annotating the Dataset

The minimal strategies require annotations about which servers and tools each task needs. Here's the workflow:

### Step 1: Run with copilot or directory strategy

```bash
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=copilot \
  --log-dir logs/copilot-run
```

### Step 2: Extract success annotations

```bash
python scripts/extract_success_logs.py logs/copilot-run -o annotations.json
```

This extracts which servers and tools were used in successful task completions.

### Step 3: Apply annotations to dataset

```bash
python scripts/annotate_dataset.py annotations.json
```

This adds `required_servers` and `required_tools` fields to each task in the dataset.

### Step 4: Run with minimal strategies

```bash
bench eval progressivemcpbench --model openai/gpt-4o -T strategy=minimal-servers
```

## Expected Results

Different strategies should yield different performance characteristics:

| Strategy | Tool Discovery | Distractions | Expected Performance |
|----------|---------------|--------------|---------------------|
| copilot | Semantic search | All tools via retrieval | Baseline |
| directory | Explicit exploration | All tools browsable | Similar to copilot |
| minimal-servers | None | Server-level | Better than baseline |
| minimal-tools | None | None | Best possible |
| distraction-128 | None | 128 total tools | Measures distraction resistance |

## CLI Options

```bash
bench eval progressivemcpbench --model <model> -T strategy=<strategy> [options]

# Required:
-T strategy=<copilot|directory|minimal-servers|minimal-tools|distraction-128>

# Common options:
--limit N          # Run only N tasks
--epochs N         # Run each task N times
--epochs-reducer   # How to combine epoch scores (mean, max, etc.)
--alpha            # Use alpha benchmarks
```

## Understanding the Benchmark

The core research question ProgressiveMCPBench addresses is: **How does tool discovery mechanism affect agent performance?**

- **Copilot** tests whether semantic search can effectively surface relevant tools
- **Directory** tests whether explicit browsing is more or less effective than semantic search
- **Minimal-servers** tests the real-world scenario of attaching the right MCP server
- **Minimal-tools** establishes an upper bound on performance with perfect tool selection
- **Distraction-128** tests robustness to irrelevant tools in the context

## Scoring

Tasks are scored using exact and fuzzy string matching against expected answers:
- **Exact match (1.0)**: Answer matches one of the acceptable answers
- **Fuzzy match (0.5)**: Answer is similar enough (≥85% similarity)
- **No match (0.0)**: Answer doesn't match

The model must output a JSON object with a `final_answer` field.

## Requirements

- OPENAI_API_KEY (for embeddings in copilot strategy)
- Node.js (for running MCP servers)
- Various MCP server dependencies (automatically managed)

## References

Based on [LiveMCPBench](https://github.com/icip-cas/LiveMCPBench) with extensions for progressive tool discovery research.
