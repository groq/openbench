# Example sweep config: AIME 2025 II across GPT-OSS 120B single vs. pro-mode fanout
name: "supergpqa_pro_mode_sweep_120b"
log_dir: "./logs"

benchmarks:
  - name: "supergpqa"

runs:
  # Baseline single-generation 120B on Groq
  - label: "120b-single"
    model: "groq/openai/gpt-oss-120b"

  # Pro mode (pyfunc fan-out + synth) with varying n_runs, all using GPT-OSS-120B underneath
  - label: "pro-2x-120b"
    model: "pyfunc/openbench.examples.groq_gpt_oss_pro:gpt_5_pro_mode"
    model_args:
      n_runs: 2
      model: "openai/gpt-oss-120b"

  - label: "pro-4x-120b"
    model: "pyfunc/openbench.examples.groq_gpt_oss_pro:gpt_5_pro_mode"
    model_args:
      n_runs: 4
      model: "openai/gpt-oss-120b"

  - label: "pro-8x-120b"
    model: "pyfunc/openbench.examples.groq_gpt_oss_pro:gpt_5_pro_mode"
    model_args:
      n_runs: 8
      model: "openai/gpt-oss-120b"

  - label: "pro-12x-120b"
    model: "pyfunc/openbench.examples.groq_gpt_oss_pro:gpt_5_pro_mode"
    model_args:
      n_runs: 12
      model: "openai/gpt-oss-120b"

shared:
  # Common eval knobs (override here if desired)
  epochs: 1
  limit: "0,300"   # uncomment to run only the first 300 problems, or set a single int
  temperature: 0.6
  top_p: 1.0
  # max_tokens: 8000   # optional global cap
  timeout: 10000